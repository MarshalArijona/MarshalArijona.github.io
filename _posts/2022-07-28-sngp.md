---
layout: post
title: A Review of Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness (Under Construction)
date: 2022-07-28 11:12:00-0400
description:
tags: Bayesian-neural-network uncertainty-quantification Gaussian-process spectral-normalization
categories: machine-learning-posts
---


#### Proof of preposition 1

Let $$I(\mathbf{x}) = \mathbf{x}$$ s.t. for $$h(\mathbf{x}) = \mathbf{x} + g(\mathbf{x})$$, we can write $$g = h - I$$. Given a function $$h : \mathcal{X} \rightarrow \mathcal{H}$$, we have $$\|h\| = \sup \frac{\|h(\mathbf{x})\|_H}{\|\mathbf{x}\|_X}$$, for $$\mathbf{x} \in \mathcal{X}, \, \mathbf{x} > 0$$. Furthermore, we also denote the Lipschitz seminorm for a function as:

\begin{equation}
\Vert h \Vert_L = \sup \frac{\Vert h(\mathbf{x} - h(\mathbf{x}^\prime)\Vert_H}{\Vert\mathbf{x} - \mathbf{x}^\prime)\Vert_X}
\end{equation}

where $$\mathbf{x}, \mathbf{x}^\prime \in \mathcal{X}$$ and $$\mathbf{x}^\prime \neq \mathbf{x}$$. We can express Lipschitz inequality of $$\mathbf{x}$$ and $$\mathbf{x}^\prime$$ as

\begin{equation}
\Vert h(\mathbf{x}) - h(\mathbf{x}^\prime)\Vert_H \leq \alpha \Vert \mathbf{x} - \mathbf{x}^\prime  \Vert 
\end{equation}

Note that the inequality above is symmetric s.t. $$\Vert h(\mathbf{x}^\prime) - h(\mathbf{x})\Vert_H \leq \alpha \Vert \mathbf{x}^\prime - \mathbf{x} \Vert$$. Furthermore, let us assume $$\forall\, l \; \Vert g_l \Vert_L = \Vert h_l - I \Vert_L < \alpha < 1$$. We will show that

\begin{equation}\label{eq:bilipschitz-resnet}
(1 - \alpha) \Vert \mathbf{x} - \mathbf{x}^\prime \Vert \leq \Vert h_l(\mathbf{x}) - h_l(\mathbf{x}^\prime) \Vert \leq (1 + \alpha) \Vert \mathbf{x} - \mathbf{x}^\prime \Vert
\end{equation}

The inequality above is the bi-Lipschitz condition for residual neural network. Let us examine the LHS first

$$
\begin{eqnarray}
\Vert \mathbf{x} - \mathbf{x}^\prime \Vert &&\leq \Vert \mathbf{x} - \mathbf{x}^\prime - (h_l(\mathbf{x}) - h_l(\mathbf{x}^\prime)) + (h_l(\mathbf{x}) - h_l(\mathbf{x}^\prime)) \Vert \nonumber \\
&&\leq \Vert (h_l(\mathbf{x}^\prime) - \mathbf{x}^\prime) - (h_l(\mathbf{x}) - \mathbf{x}) \Vert + \Vert h_l(\mathbf{x}) - h_l(\mathbf{x}^\prime) \Vert \nonumber \\
&&\leq \Vert g_l(\mathbf{x}^\prime) - g_l(\mathbf{x}) \Vert + \Vert h_l(\mathbf{x}) - h_l(\mathbf{x}^\prime) \Vert \nonumber \\
&& \leq \alpha \Vert \mathbf{x} - \mathbf{x}^\prime \Vert + \Vert h_l(\mathbf{x}) - h_l(\mathbf{x}^\prime) \Vert \nonumber \\
(1 - \alpha) \Vert \mathbf{x} - \mathbf{x}^\prime \Vert &&\leq \Vert h_l(\mathbf{x}) - h_l(\mathbf{x}^\prime) \label{eq:LHS}
\end{eqnarray}
$$

We obtained the last row by applying $$\Vert g_l \Vert_L < \alpha$$. Now, we solve the RHS

\begin{equation} \label{eq:RHS}
\Vert h_l(\mathbf{x}) - h_l(\mathbf{x}^\prime) \Vert = \Vert \mathbf{x} + g_l(\mathbf{x}) - \mathbf{x}^\prime - g_l(\mathbf{x}^\prime) \Vert \leq \alpha \Vert \mathbf{x} - \mathbf{x}^\prime \Vert + \Vert \mathbf{x} - \mathbf{x}^\prime \Vert \leq (1 + \alpha) \Vert \mathbf{x} - \mathbf{x}^\prime \Vert
\end{equation}


Now, combining \eqref{eq:LHS} and \eqref{eq:RHS}, we obtain \eqref{eq:bilispchitz-resnet}. We can apply proof by induction to show the inequality still holds for $$L$$ layers of residual network, that is

\begin{equation}
(1 - \alpha)^L Vert \mathbf{x} - \mathbf{x}^\prime \Vert \leq \Vert h_l(\mathbf{x}) - h_l(\mathbf{x}^\prime) \Vert \leq (1 + \alpha)^L \Vert \mathbf{x} - \mathbf{x}^\prime \Vert
\end{equation}
